{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OhJunYoung21/artificial-inteliigence/blob/main/Lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC91mF_u6o-s"
      },
      "source": [
        "# Machine Learning - Lab Session 3\n",
        "\n",
        "This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. \n",
        "\n",
        "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem.\n",
        "\n",
        "### Submitting your work:\n",
        "- <font color=red>**DO NOT clear the final outputs**</font> so that TAs can grade both your code and results.\n",
        "- Commit the `.ipynb` file on your github repository and submit URL on E-Ruri"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5bL80Gdm-7TI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C62c6fKX98aM"
      },
      "source": [
        "## Preparing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnm87-pp6o-v"
      },
      "source": [
        "### Load MNIST Data\n",
        "We temperaly use `sklearng.datasets` for load MNIST before we learning `pytorch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "both",
        "id": "JLpLa8Jt7Vu4"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n",
        "X = X / 255.0\n",
        "y = y.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "pv_1sPkqByzq",
        "outputId": "d4eb1383-fc1f-444f-e954-cf4cc8d9e60f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(X[0].reshape((28, 28)), cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7aHrm6nGDMB"
      },
      "source": [
        "### Preprocessing\n",
        "Reformat into a shape that's more adapted to the models we're going to train:\n",
        "1. labels as float 1-hot encodings. [hint, use [`np.eye`](https://numpy.org/devdocs/reference/generated/numpy.eye.html)] **[pts. 5]**\n",
        "2. split the data into train and test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7TnyPMF_CuM",
        "outputId": "80ad44bd-c22e-4d8b-84e1-46d7f085a4a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before encoding: 5\n",
            "after encoding: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# one-hot encoding\n",
        "print('before encoding: %s'% y[0])\n",
        "y = np.eye(10)[y]   #np.eye 함수를 사용해서 one_hot encoding을 해줍니다.\n",
        "print('after encoding: %s' % y[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRSyYiIIGIzS",
        "outputId": "5bad88dc-3777-43f2-b579-2655ecc57cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set (56000, 784) (56000, 10)\n",
            "Test set (14000, 784) (14000, 10)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into train partition and test partition\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, random_state=0, test_size=0.2)\n",
        "\n",
        "IMAGE_SIZE = 28\n",
        "NUM_LABELS = 10\n",
        "\n",
        "print('Training set', X_train.shape, y_train.shape)\n",
        "print('Test set', X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lw7LP8x_OWi"
      },
      "source": [
        "In this lab session, we only use partial data for training and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jNaDkIi46o-y"
      },
      "outputs": [],
      "source": [
        "DATA_SIZE = 100\n",
        "\n",
        "X_train = X_train[0:DATA_SIZE]\n",
        "y_train = y_train[0:DATA_SIZE]\n",
        "\n",
        "X_test = X_test[0:DATA_SIZE]\n",
        "y_test = y_test[0:DATA_SIZE]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu-bNfMf6o-z"
      },
      "source": [
        "## Training a Neural Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewBirlzP6o-1"
      },
      "source": [
        "### Activation Function\n",
        "\n",
        "First let's implement two activation function, `sigmoid` and `softmax`. Also the derivation of `sigmoid` function. **[pts. 5 per function]**\n",
        "$$\n",
        "\\begin{align}\n",
        "\\sigma(x) &=  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-x} } \\\\\n",
        "\\mathrm{softmax}(x_{i}) &= \\frac{e^{x_i}}{\\sum_j e^{x_j}} \\\\\n",
        "\\sigma'(x) &= \\sigma(x)\\sigma(1-x) \\\\\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "m_9pl55P6o-1"
      },
      "outputs": [],
      "source": [
        "def sigmoid(Z):\n",
        "    Z = 1/(1+np.exp(-Z))\n",
        "    return Z\n",
        "\n",
        "def softmax(Z):    \n",
        "    exp_a = np.exp(Z)\n",
        "    sum_exp_a = np.sum(exp_a)\n",
        "    y = exp_a / sum_exp_a\n",
        "    \n",
        "    return y\n",
        "\n",
        "def sigmoid_derivative(Z):\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    return s*(1-s)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#def softmax(Z):\n",
        "  #return np.exp(Z)/np.exp(Z).sum(aixs=0,keepdims=True)"
      ],
      "metadata": {
        "id": "sTHaNeQjtYx9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpeOlzYaAjVU",
        "outputId": "2f0b8881-cdd5-4b90-c260-4d8fdc6b65ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.52497918747894\n",
            "[0.26175419 0.50140083 0.23684498]\n",
            "0.24937604019289197\n"
          ]
        }
      ],
      "source": [
        "# test functions\n",
        "print(sigmoid(0.1))\n",
        "print(softmax([0.15, 0.8, 0.05]))\n",
        "print(sigmoid_derivative(0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH3YKCDC6o-z"
      },
      "source": [
        "Let's now build a neural network with one input layer, one hidden layer, and one output layer. The number of nodes in the input layer is determined by the dimensionality of our data, 784. Similarly, the number of nodes in the output layer is determined by the number of classes we have, 10. The input to the network will be the pixel values of the input image and its output will be ten probabilities, ones for each class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B1bQjCm_xkL"
      },
      "source": [
        "![Sample network](https://drive.google.com/uc?id=1D1XuhvNokK5S5yn8V34JGvyE-XXz9rST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObPwuWqv6o-z"
      },
      "source": [
        "### How our network makes predictions\n",
        "\n",
        "Our network makes predictions using *forward propagation*, which is just a bunch of matrix multiplications and the application of the activation function(s) we defined above. If $x$ is the 784-dimensional input to our network then we calculate our prediction $\\hat{y}$ (ten-dimensional) as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJBKF3zf6o-z"
      },
      "source": [
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{s}_1 & = W_1\\mathbf{x} + \\mathbf{b}_1 \\\\\n",
        "\\mathbf{z}_1 & = \\mathrm{sigmoid}(\\mathbf{s}_1) \\\\\n",
        "\\mathbf{s}_2 & = W_2\\mathbf{z}_1 + \\mathbf{b}_2 \\\\\n",
        "\\mathbf{h} & = \\hat{y} = \\mathrm{softmax}(\\mathbf{s}_2)\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG3J5N6A6o-0"
      },
      "source": [
        "$z_i$ is the input of layer $i$ and $a_i$ is the output of layer $i$ after applying the activation function. $W_1, b_1, W_2, b_2$ are  parameters of our network, which we need to learn from our training data. You can think of them as matrices transforming data between layers of the network. Looking at the matrix multiplications above we can figure out the dimensionality of these matrices.\n",
        "\n",
        "If we use 1024 nodes for our hidden layer then $W_1 \\in \\mathbb{R}^{1024\\times784}$, $b_1 \\in \\mathbb{R}^{1024\\times1}$, $W_2 \\in \\mathbb{R}^{10\\times1024}$, $b_2 \\in \\mathbb{R}^{10\\times1}$. Now you see why we have more parameters if we increase the size of the hidden layer.\n",
        "\n",
        "**[note]** beware of dimension of $\\mathbf{x}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pVFqhiNb_prh"
      },
      "outputs": [],
      "source": [
        "layers_size = [784, 50, 10]   #입력이 784, hidden layer가 50, output이 10개이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDjwqr-uIXM6"
      },
      "source": [
        "Define parameters and initialize them with `np.random.rand` for W and `np.zeros` for b. **[pts. 5]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayJva00z_oB2",
        "outputId": "7f3f88ed-2cf4-4710-c9ba-53232cd45be2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 784)\n",
            "(50, 1)\n",
            "(10, 50)\n",
            "(10, 1)\n"
          ]
        }
      ],
      "source": [
        "# initialization\n",
        "np.random.seed(1)\n",
        "\n",
        "parameters = {}\n",
        "parameters[\"W1\"] = np.random.randn(layers_size[1], layers_size[0]) / np.sqrt(layers_size[0])\n",
        "parameters[\"b1\"] = np.zeros((layers_size[1], 1))\n",
        "\n",
        "parameters[\"W2\"] = np.random.randn(layers_size[2], layers_size[1]) / np.sqrt(layers_size[1])\n",
        "parameters[\"b2\"] = np.zeros((layers_size[2], 1))\n",
        "print(parameters[\"W1\"].shape)\n",
        "print(parameters[\"b1\"].shape)\n",
        "print(parameters[\"W2\"].shape)\n",
        "print(parameters[\"b2\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed forward $\\mathbf{x}$ through Networks. **[pts. 10]**"
      ],
      "metadata": {
        "id": "lorKSUwDYoNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vD-7B0fknV2",
        "outputId": "3c55d3ee-f15c-4f91-cc66-2249ffd02c4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 100)\n",
            "(10, 100)\n"
          ]
        }
      ],
      "source": [
        "# forward\n",
        "store = {}\n",
        "store['X'] = X_train.T   #transpose를 한번 해서 차원을 맞춰주는 역할입니다.\n",
        "\n",
        "## input to hidden\n",
        "S = parameters['W1'].dot(store['X']) + parameters['b1']  #S1 = w1*x1 + b1형태로 만들어 주었습니다.\n",
        "Z = sigmoid(S)\n",
        "store[\"Z\"] = Z\n",
        "store[\"W1\"] = parameters[\"W1\"]\n",
        "store[\"S1\"] = S\n",
        "\n",
        "## hidden to output\n",
        "S = parameters['W2'].dot(Z) + parameters['b2']\n",
        "H = softmax(S)   #우리의 목적은 언제까지나 확률을 구하는 것이기 때문에 softmax함수를 사용합니다.\n",
        "store[\"H\"] = H        \n",
        "store[\"W2\"] = parameters[\"W2\"]\n",
        "store[\"S2\"] = S      \n",
        "\n",
        "print(store[\"Z\"].shape)\n",
        "print(store[\"H\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtkfchpT6o-0"
      },
      "source": [
        "### Learning the Parameters\n",
        "\n",
        "Learning the parameters for our network means finding parameters ($W_1, b_1, W_2, b_2$) that minimize the error on our training data. But how do we define the error? We call the function that measures our error the *loss function*. A common choice with the softmax output is the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression). If we have $N$ training examples and $C$ classes then the loss for our prediction $\\hat{y}$ with respect to the true labels $y$ is given by:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "L(y,\\hat{y}) = - \\frac{1}{N} \\sum_{n \\in N} \\sum_{i \\in C} y_{n,i} \\log\\hat{y}_{n,i}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIiOvIyZ6o-0"
      },
      "source": [
        "The formula looks complicated, but all it really does is sum over our training examples and add to the loss if we predicted the incorrect class. So, the further away $y$ (the correct labels) and $\\hat{y}$ (our predictions) are, the greater our loss will be. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldCES_Px6o-0"
      },
      "source": [
        "Remember that our goal is to find the parameters that minimize our loss function. We can use [gradient descent](http://cs231n.github.io/optimization-1/) to find its minimum. I will implement the most vanilla version of gradient descent, also called batch gradient descent with a fixed learning rate. Variations such as SGD (stochastic gradient descent) or minibatch gradient descent typically perform better in practice. So if you are serious you'll want to use one of these, and ideally you would also [decay the learning rate over time](http://cs231n.github.io/neural-networks-3/#anneal).\n",
        "\n",
        "As an input, gradient descent needs the gradients (vector of derivatives) of the loss function with respect to our parameters: $\\frac{\\partial{L}}{\\partial{W_1}}$, $\\frac{\\partial{L}}{\\partial{b_1}}$, $\\frac{\\partial{L}}{\\partial{W_2}}$, $\\frac{\\partial{L}}{\\partial{b_2}}$. To calculate these gradients we use the famous *backpropagation algorithm*, which is a way to efficiently calculate the gradients starting from the output. I won't go into detail how backpropagation works, but there are many excellent explanations ([here](http://colah.github.io/posts/2015-08-Backprop/) or [here](http://cs231n.github.io/optimization-2/)) floating around the web."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMCgWwWb6o-0"
      },
      "source": [
        "Applying the backpropagation formula we find the following (trust me on this): **[pts. 20]**\n",
        "\n",
        "backpropagation for output layer\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\mathbf{e} = \\hat{y} - y \\\\\n",
        "& \\delta_2 = \\mathbf{e} \\sigma'(\\mathbf{s}_2) \\\\\n",
        "& \\frac{\\partial{L}}{\\partial{W_2}} = \\mathbf{z}^T \\delta_2  \\\\\n",
        "& \\frac{\\partial{L}}{\\partial{b_2}} = \\delta_2 \\\\ \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "backpropagation for hidden layer\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\delta_1 = W_2^T\\delta_2 \\circ \\sigma'(\\mathbf{s}_1)  \\\\\n",
        "& \\frac{\\partial{L}}{\\partial{W_1}} = \\mathbf{x}^T \\delta_1\\\\\n",
        "& \\frac{\\partial{L}}{\\partial{b_1}} = \\delta_1 \\\\\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJnCUVgepcx0",
        "outputId": "bc7c596e-3cdb-4c35-dab2-09e49f83b4f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 784)\n",
            "(50, 1)\n",
            "(10, 50)\n",
            "(10, 1)\n"
          ]
        }
      ],
      "source": [
        "n = X_train.shape[0]\n",
        "derivatives = {}\n",
        "\n",
        "# backward\n",
        "## output to hidden\n",
        "error = store['H'] - y_train.T  #H는 output입니다.\n",
        "deleta2 = error*sigmoid_derivative(store['S2'])  #error 곱하기 sigmoid를 미분한 함수에 s2를 집어넣습니다.\n",
        "\n",
        "dW = deleta2.dot(store['Z'].T)\n",
        "db = np.sum(deleta2, axis=1, keepdims=True)\n",
        "\n",
        "derivatives[\"dW2\"] = dW\n",
        "derivatives[\"db2\"] = db\n",
        "\n",
        "## hidden to output\n",
        "dAPrev = store['W2'].T.dot(deleta2)\n",
        "deleta1 = dAPrev * sigmoid_derivative(store['S1'])\n",
        "\n",
        "dW = deleta1.dot(store['X'].T)\n",
        "db = np.sum(deleta1, axis=1, keepdims=True)\n",
        "\n",
        "derivatives[\"dW1\"] = dW\n",
        "derivatives[\"db1\"] = db\n",
        "\n",
        "print(derivatives['dW1'].shape)\n",
        "print(derivatives['db1'].shape)\n",
        "print(derivatives['dW2'].shape)\n",
        "print(derivatives['db2'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jw22p14wap8"
      },
      "source": [
        "### Gradient Descent\n",
        "Update via gradient descent as follows: **[pts. 10]**\n",
        "$$\n",
        "\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta ∇\\frac{\\partial \\varepsilon}{\\partial\\mathbf{w}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hkt5YtKAu_Zd"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "parameters[\"W2\"] = parameters['W2'] - learning_rate*derivatives['dW2']\n",
        "parameters[\"b2\"] =  parameters['b2'] - learning_rate*derivatives['db2']\n",
        "\n",
        "parameters[\"W1\"] = parameters['W1'] - learning_rate*derivatives['dW1']\n",
        "parameters[\"b1\"] =  parameters['b1'] - learning_rate*derivatives['db1']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjXVLPGE6o-0"
      },
      "source": [
        "## Implementation of Whole Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pUWHI8x6o-1"
      },
      "source": [
        "Finally, here comes the function to train our Artificial Neural Network as class. Complete the code. **[pts. 30]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "-nIqBuT66o-1"
      },
      "outputs": [],
      "source": [
        "class ANN:\n",
        "    def __init__(self, layers_size):\n",
        "        self.layers_size = layers_size\n",
        "        self.parameters = {}\n",
        "        self.L = len(self.layers_size)\n",
        "        self.n = 0\n",
        "        self.costs = []\n",
        "        self.val_costs = []\n",
        " \n",
        "    def sigmoid(self, Z):\n",
        "        Z = 1/(1+np.exp(-Z))\n",
        "        return Z\n",
        " \n",
        "    def softmax(self, Z):\n",
        "       exp_a = np.exp(Z)\n",
        "       sum_exp_a = np.sum(exp_a)\n",
        "       y = exp_a / sum_exp_a\n",
        "    \n",
        "       return y\n",
        "\n",
        "    def sigmoid_derivative(self, Z):\n",
        "        s = 1/(1+np.exp(-Z))\n",
        "        return s*(1-s)\n",
        " \n",
        "    def initialize_parameters(self):\n",
        "        np.random.seed(1)\n",
        "        self.parameters = {}\n",
        "        self.parameters[\"W1\"] = np.random.randn(self.layers_size[1], self.layers_size[0]) / np.sqrt(self.layers_size[0])\n",
        "        self.parameters[\"b1\"] = np.zeros((self.layers_size[1], 1))\n",
        "\n",
        "        self.parameters[\"W2\"] = np.random.randn(self.layers_size[2], self.layers_size[1]) / np.sqrt(self.layers_size[1])  \n",
        "        self.parameters[\"b2\"] = np.zeros((self.layers_size[2], 1))\n",
        "\n",
        "    def forward(self, X):\n",
        "        store = {}\n",
        "        store['X'] = X.T\n",
        "        \n",
        "        ########### complete the code ################\n",
        "        ## input to hidden\n",
        "        S = self.parameters['W1'].dot(store['X']) + self.parameters['b1']  #S1 = w1*x1 + b1형태로 만들어 주었습니다. \n",
        "        Z = sigmoid(S)\n",
        "        store[\"Z\"] = Z\n",
        "        store[\"W1\"] = self.parameters[\"W1\"]\n",
        "        store[\"S1\"] = S\n",
        "\n",
        "        ## hidden to output\n",
        "        S = self.parameters['W2'].dot(Z) + self.parameters['b2']\n",
        "        H = softmax(S)   #우리의 목적은 언제까지나 확률을 구하는 것이기 때문에 softmax함수를 사용합니다.\n",
        "        store[\"H\"] = H        \n",
        "        store[\"W2\"] = self.parameters[\"W2\"]\n",
        "        store[\"S2\"] = S     \n",
        "        ##############################################\n",
        "        return H, store\n",
        " \n",
        "    def backward(self, X, Y, store):\n",
        "        ########### complete the code ################\n",
        "        n = X_train.shape[0]\n",
        "        derivatives = {}\n",
        "\n",
        "        # backward\n",
        "        ## output to hidden\n",
        "        error = store['H'] - Y.T\n",
        "        deleta2 = error*sigmoid_derivative(store['S2']) \n",
        "\n",
        "        dW = deleta2.dot(store['Z'].T)\n",
        "        db = np.sum(deleta2, axis=1, keepdims=True)\n",
        "\n",
        "        derivatives[\"dW2\"] = dW\n",
        "        derivatives[\"db2\"] = db\n",
        "\n",
        "        ## hidden to output\n",
        "        dAPrev = store['W2'].T.dot(deleta2)\n",
        "        deleta1 = dAPrev * sigmoid_derivative(store['S1'])\n",
        "\n",
        "        dW = deleta1.dot(store['X'].T)\n",
        "        db = np.sum(deleta1, axis=1, keepdims=True)\n",
        "\n",
        "        derivatives[\"dW1\"] = dW\n",
        "        derivatives[\"db1\"] = db\n",
        "        ##############################################\n",
        " \n",
        "        return derivatives\n",
        " \n",
        "    def fit(self, X, Y, learning_rate=0.01, n_iterations=1000):\n",
        "        np.random.seed(1)\n",
        " \n",
        "        self.n = X.shape[0]\n",
        "        self.layers_size.insert(0, X.shape[1])\n",
        "        self.initialize_parameters()\n",
        "\n",
        "        for loop in range(n_iterations):\n",
        "            H, store = self.forward(X)\n",
        "            cost = -np.mean(Y * np.log(H.T+ 1e-8))\n",
        "            derivatives = self.backward(X, Y, store)\n",
        " \n",
        "            for l in range(1, self.L + 1):\n",
        "                # gradient descent\n",
        "                ########### complete the code ################\n",
        "                self.parameters[\"W2\"] -= learning_rate*derivatives['dW2']\n",
        "                self.parameters[\"b2\"] -= learning_rate*derivatives['db2']\n",
        "\n",
        "                self.parameters[\"W1\"] -= learning_rate*derivatives['dW1']\n",
        "                self.parameters[\"b1\"] -= learning_rate*derivatives['db1']\n",
        "                ##############################################\n",
        "\n",
        "            if loop % 10 == 0:\n",
        "                print(\"[%3d/%3d] Cost: %.4f\"%(loop, n_iterations, cost),\n",
        "                      \"Train Accuracy: %2.2f\"%(self.predict(X, Y)))\n",
        " \n",
        "            self.costs.append(cost)\n",
        "            H, store = self.forward(X_test)\n",
        "            val_cost = -np.mean(y_test * np.log(H.T+ 1e-8))            \n",
        "            self.val_costs.append(val_cost)\n",
        " \n",
        "    def predict(self, X, Y):\n",
        "        A, cache = self.forward(X)\n",
        "        y_hat = np.argmax(A, axis=0)\n",
        "        Y = np.argmax(Y, axis=1)\n",
        "        accuracy = (y_hat == Y).mean()\n",
        "        return accuracy * 100\n",
        " \n",
        "    def plot_cost(self):\n",
        "        plt.figure()\n",
        "        plt.plot(np.arange(len(self.costs)), self.costs)\n",
        "        plt.plot(np.arange(len(self.val_costs)), self.val_costs)\n",
        "        plt.xlabel(\"epochs\")\n",
        "        plt.ylabel(\"cost\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yTl-FzF9Jqg"
      },
      "source": [
        "Build Model with 50 number of hidden units and train it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "_ta25z2B5Cm0",
        "outputId": "e593a994-b3d6-4cf6-8df5-69f835d89f72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  0/200] Cost: 0.7082 Train Accuracy: 10.00\n",
            "[ 10/200] Cost: 0.6736 Train Accuracy: 20.00\n",
            "[ 20/200] Cost: 0.6739 Train Accuracy: 23.00\n",
            "[ 30/200] Cost: 0.6745 Train Accuracy: 26.00\n",
            "[ 40/200] Cost: 0.6752 Train Accuracy: 27.00\n",
            "[ 50/200] Cost: 0.6757 Train Accuracy: 30.00\n",
            "[ 60/200] Cost: 0.6762 Train Accuracy: 32.00\n",
            "[ 70/200] Cost: 0.6766 Train Accuracy: 32.00\n",
            "[ 80/200] Cost: 0.6770 Train Accuracy: 32.00\n",
            "[ 90/200] Cost: 0.6774 Train Accuracy: 32.00\n",
            "[100/200] Cost: 0.6777 Train Accuracy: 31.00\n",
            "[110/200] Cost: 0.6780 Train Accuracy: 32.00\n",
            "[120/200] Cost: 0.6783 Train Accuracy: 32.00\n",
            "[130/200] Cost: 0.6786 Train Accuracy: 32.00\n",
            "[140/200] Cost: 0.6788 Train Accuracy: 34.00\n",
            "[150/200] Cost: 0.6791 Train Accuracy: 34.00\n",
            "[160/200] Cost: 0.6793 Train Accuracy: 34.00\n",
            "[170/200] Cost: 0.6795 Train Accuracy: 34.00\n",
            "[180/200] Cost: 0.6797 Train Accuracy: 34.00\n",
            "[190/200] Cost: 0.6799 Train Accuracy: 34.00\n",
            "Train Accuracy: 34.0000\n",
            "Test Accuracy: 31.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "layers_dims = [50, 10]\n",
        "\n",
        "ann = ANN(layers_dims)\n",
        "ann2 = ANN(layers_dims)\n",
        "ann.fit(X_train, y_train, learning_rate=.01, n_iterations=200)\n",
        "print(\"Train Accuracy: %.4f\" % ann.predict(X_train, y_train))\n",
        "print(\"Test Accuracy: %.4f\" % ann.predict(X_test, y_test))\n",
        "ann.plot_cost()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OcVcuGWz54fD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Lab 3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}